{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d15d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from flask_cors import CORS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images: List[bytes], labels: List[int], transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(io.BytesIO(self.images[idx])).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 28 * 28, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.classes = []\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        images: List[bytes],\n",
    "        labels: List[str],\n",
    "        epochs: int,\n",
    "        on_epoch_end: Callable[[int, float, float], None]\n",
    "    ) -> Dict[str, float]:\n",
    "        self.classes = sorted(list(set(labels)))\n",
    "        label_to_idx = {label: idx for idx, label in enumerate(self.classes)}\n",
    "        numeric_labels = [label_to_idx[label] for label in labels]\n",
    "\n",
    "        dataset = ImageDataset(images, numeric_labels, self.transform)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size for speed\n",
    "        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "        self.model = SimpleCNN(len(self.classes)).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for images_batch, labels_batch in train_loader:\n",
    "                images_batch = images_batch.to(self.device)\n",
    "                labels_batch = labels_batch.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += labels_batch.size(0)\n",
    "                train_correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "            train_accuracy = train_correct / train_total\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images_batch, labels_batch in val_loader:\n",
    "                    images_batch = images_batch.to(self.device)\n",
    "                    labels_batch = labels_batch.to(self.device)\n",
    "\n",
    "                    outputs = self.model(images_batch)\n",
    "                    loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels_batch.size(0)\n",
    "                    val_correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "            val_accuracy = val_correct / val_total if val_total > 0 else 0.0\n",
    "\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "\n",
    "            on_epoch_end(epoch + 1, train_loss / len(train_loader), train_accuracy)\n",
    "\n",
    "        return {\n",
    "            'final_accuracy': best_val_acc,\n",
    "            'classes': self.classes\n",
    "        }\n",
    "\n",
    "    def predict(self, image_bytes: bytes) -> List[Dict[str, float]]:\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            probs = probabilities[0].cpu().numpy()\n",
    "\n",
    "        results = [\n",
    "            {'class': class_name, 'confidence': float(prob)}\n",
    "            for class_name, prob in zip(self.classes, probs)\n",
    "        ]\n",
    "        return sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        # In in-memory mode we avoid creating files on disk.\n",
    "        # If you need persistence, use an explicit disk save utility.\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        # no-op: intentionally do not write to disk\n",
    "        return None\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        # Disk loading is disabled in in-memory mode.\n",
    "        raise RuntimeError(\"Loading from disk is disabled in the in-memory runner\")\n",
    "\n",
    "\n",
    "app = Flask(__name__, static_folder='static')\n",
    "CORS(app)\n",
    "\n",
    "# Supabase removed — use a local file-backed model registry instead\n",
    "# In-memory registry — keep everything in RAM and avoid creating files\n",
    "MODELS_META = []  # list of metadata dicts, newest first\n",
    "MODELS_IN_MEMORY = {}  # name -> {'state_dict': ..., 'classes': [...], ...}\n",
    "\n",
    "def _load_models_meta():\n",
    "    return MODELS_META\n",
    "\n",
    "\n",
    "def _save_models_meta(records):\n",
    "    # no-op for disk; keep meta in memory\n",
    "    global MODELS_META\n",
    "    MODELS_META = list(records)\n",
    "\n",
    "\n",
    "def add_model_meta(record):\n",
    "    MODELS_META.insert(0, record)\n",
    "    # keep a copy in the in-memory registry for lookups\n",
    "    name = record.get('name')\n",
    "    if name:\n",
    "        MODELS_IN_MEMORY[name] = {\n",
    "            'state_dict': None,\n",
    "            'classes': record.get('classes', []),\n",
    "            'meta': record,\n",
    "        }\n",
    "\n",
    "classifier = ImageClassifier()\n",
    "training_metrics = []\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return send_from_directory('static', 'index.html')\n",
    "\n",
    "\n",
    "@app.route('/api/train', methods=['POST'])\n",
    "def train_model():\n",
    "    global training_metrics\n",
    "    training_metrics = []\n",
    "\n",
    "    try:\n",
    "        files = request.files.getlist('images')\n",
    "        labels = request.form.getlist('labels')\n",
    "        epochs = int(request.form.get('epochs', 3))  # Reduced default epochs for speed\n",
    "        model_name = request.form.get('model_name', 'image-classifier')\n",
    "\n",
    "        if len(files) < 2:\n",
    "            return jsonify({'error': 'At least 2 images required'}), 400\n",
    "\n",
    "        if len(set(labels)) < 2:\n",
    "            return jsonify({'error': 'At least 2 different labels required'}), 400\n",
    "\n",
    "        image_bytes = [file.read() for file in files]\n",
    "\n",
    "        def on_epoch_end(epoch: int, loss: float, accuracy: float):\n",
    "            metric = {\n",
    "                'epoch': epoch,\n",
    "                'loss': loss,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "            training_metrics.append(metric)\n",
    "            print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "\n",
    "        result = classifier.train(image_bytes, labels, epochs, on_epoch_end)\n",
    "\n",
    "        # Keep model in memory instead of creating files on disk\n",
    "        try:\n",
    "            # persist the trained state in the in-memory registry\n",
    "            saved_state = classifier.model.state_dict() if getattr(classifier, 'model', None) is not None else None\n",
    "            MODELS_IN_MEMORY[model_name] = {\n",
    "                'state_dict': saved_state,\n",
    "                'classes': result['classes'],\n",
    "                'accuracy': float(result['final_accuracy']),\n",
    "                'epochs': epochs,\n",
    "                'total_images': len(files),\n",
    "                'created_at': datetime.utcnow().isoformat()\n",
    "            }\n",
    "            add_model_meta({\n",
    "                'name': model_name,\n",
    "                'classes': result['classes'],\n",
    "                'accuracy': float(result['final_accuracy']),\n",
    "                'epochs': epochs,\n",
    "                'total_images': len(files),\n",
    "                'created_at': datetime.utcnow().isoformat()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Model registry write error: {e}\")\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'accuracy': result['final_accuracy'],\n",
    "            'classes': result['classes'],\n",
    "            'metrics': training_metrics\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/api/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        if 'image' not in request.files:\n",
    "            return jsonify({'error': 'No image provided'}), 400\n",
    "\n",
    "        image_file = request.files['image']\n",
    "        image_bytes = image_file.read()\n",
    "\n",
    "        predictions = classifier.predict(image_bytes)\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'predictions': predictions\n",
    "        })\n",
    "\n",
    "    except ValueError as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/api/metrics', methods=['GET'])\n",
    "def get_metrics():\n",
    "    return jsonify({'metrics': training_metrics})\n",
    "\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "def get_models():\n",
    "    try:\n",
    "        records = _load_models_meta()\n",
    "        return jsonify({'models': records})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/api/load-model/<model_name>', methods=['POST'])\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        # load model from the in-memory registry\n",
    "        if model_name not in MODELS_IN_MEMORY:\n",
    "            return jsonify({'error': 'Model not found in memory'}), 404\n",
    "\n",
    "        entry = MODELS_IN_MEMORY[model_name]\n",
    "        state = entry.get('state_dict')\n",
    "        classes = entry.get('classes', [])\n",
    "\n",
    "        if state is None:\n",
    "            return jsonify({'error': 'Model weights are not available in memory'}), 404\n",
    "\n",
    "        # instantiate a new model and load the state dict\n",
    "        classifier.model = SimpleCNN(len(classes)).to(classifier.device)\n",
    "        classifier.model.load_state_dict(state)\n",
    "        classifier.model.eval()\n",
    "        classifier.classes = classes\n",
    "\n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'classes': classifier.classes\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Running in-memory mode — do not create files or directories automatically\n",
    "    # Disable Flask's auto-reloader while in debug mode to avoid double-starting\n",
    "    # (the duplicate messages and duplicate restarts come from the reloader spawning\n",
    "    # a child process on Windows). Setting `use_reloader=False` keeps the single\n",
    "    # process behavior while preserving debug output.\n",
    "    app.run(debug=True, use_reloader=False, port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
